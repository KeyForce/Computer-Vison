[TOC]

# 一、目标检测

## Cascade-RCNN

**使用不同的IOU阈值，训练多个级联的检测器**

Cascade R-CNN就是使用不同的IOU阈值，训练了多个级联的检测器，以通过调整bounding boxes，给下一阶段找到一个IoU更高的正样本来训练。

### ResNet ResNeXt

![img](J:\Main Project\Computer-Vison\MyNote\image\v2-0c5ac698702de2970016afc2834816d5_720w.jpg)



首先这是两者block的区别，左侧为ResNet的block结构，右侧的是ResNeXt的block结构。

Bottleneck architecture**这个是针对于网络结构中输入/输出的维度(dimensions)来讲的**，这三层结构是1X1,3X3和1X1卷积

ResNeXt的**分支是同构的**

### HRNet

![img](J:\Main Project\Computer-Vison\MyNote\image\v2-1d6e351cfcc9f68c163a53d80790e846_b.jpg)

将**不同分辨率的feature map进行并联与特征融合**，在并联的基础上，添加不同分辨率feature map之间的融合(fusion)。

### SE-Net

**SE-Net** 通过自适应地重新校准通道特征响应来引入**通道注意力（channel-attention）机制**。 

Squeeze-Excitation

![在这里插入图片描述](J:\Main Project\Computer-Vison\MyNote\image\20190516132213766.png)



通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征

![img](J:\Main Project\Computer-Vison\MyNote\image\247d198e8ef64a7fa040887b6f0ee0e0_th.jpg)

Reweight 的操作 Excitation 的输出的权重看做是进过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的**重标定**

### SK-Net

**SK-Net** 通过两个网络分支引入**特征图注意力（feature-map attention**）

![在这里插入图片描述](J:\Main Project\Computer-Vison\MyNote\image\20190326174632615.png)

### Shufflenet v2

**4条关于 CNN 网络结构设计的准则**

* Guideline 1(G1): 输入通道数与输出通道数保持相等可以最小化内存访问成本（memory access cost,MAC）。
* Guideline 2(G2): 分组卷积中使用过多的分组数会增加内存访问成本（MAC)
* Guideline 3(G3): 网络结构太复杂（分支和基本单元过多）会降低网络的并行程度
* Guideline 4(G4): Element-wise 的操作消耗也不可忽略（包括ReLU，Tensor的相加，偏置的相加等等操作）

**深度分离卷积（depth-wise convolution）和 分组卷积（Group convolution）**



分组卷积（Group convolution）

![img](J:\Main Project\Computer-Vison\MyNote\image\v2-90853f4dc1ebd3d3a2ea6d9651c37c80_720w-1589809493751.jpg)

深度分离卷积（depth-wise convolution）包含深度卷积和逐点卷积

![img](J:\Main Project\Computer-Vison\MyNote\image\v2-3060b36fe063bbfe99622be4a4b23a02_720w.jpg)

![img](J:\Main Project\Computer-Vison\MyNote\image\20190502131719539.png)

![img](J:\Main Project\Computer-Vison\MyNote\image\20190502131822820.png)

### MobilenetV2

* 增加Inverted residuals

  通常的residuals block是先1*1的Conv layer，减少通道数下来，再3*3 Conv layer，最后经过一个1*1 的Conv layer，将通道数再“扩张”回去。即先“压缩”，最后“扩张”回去。

  而 inverted residuals就是 先“扩张”，最后“压缩”，Depth-wise convolution之前多了一个1*1的“扩张”层，目的是为了提升通道数，获得更多特征；

![img](J:\Main Project\Computer-Vison\MyNote\image\v2-fe121511e18635a96f491ac99a6cd547_720w.jpg)

* Linear bottlenecks

  为了避免Relu对特征的破坏，在residual block的Eltwise sum之前的那个 1*1 Conv 不再采用Relu，而采用Linear，目的是防止Relu破坏特征。

## 训练技巧

### Warmup LR

### 多尺度训练

## 推理技巧

### TTA（Test Time Augmentation）

这里会为原始图像造出多个不同版本，包括不同区域裁剪和更改缩放程度等，并将它们输入到模型中；然后对多个版本进行计算得到平均输出，作为图像的最终输出分数

## 损失函数



![目标检测回归损失函数简介：SmoothL1/IoU/GIoU/DIoU/CIoU Loss](J:\Main Project\Computer-Vison\MyNote\image\v2-a7908704f0963013ee0f29807b43a940_1200x500.jpg)

### 1.IoU Loss

交并比

![[公式]](J:\Main Project\Computer-Vison\MyNote\image\equation-1589025634590.svg)

![img](J:\Main Project\Computer-Vison\MyNote\image\13942301-2776160ba9791aaa.webp)

```
import numpy as np

def Iou(box1, box2)
	xmin1, ymin1, xmax1, ymax1 = box1
	xmin2, ymin2, xmax2, ymax2 = box2
	# 获取矩形框交集对应的左上角和右下角的坐标（intersection）
    xx1 = np.max([xmin1, xmin2])
    yy1 = np.max([ymin1, ymin2])
    xx2 = np.min([xmax1, xmax2])
    yy2 = np.min([ymax1, ymax2])
    # 计算两个矩形框面积
    area1 = (xmax1-xmin1) * (ymax1-ymin1) 
    area2 = (xmax2-xmin2) * (ymax2-ymin2)
    # 计算交集面积
    inter_area = (np.max([0, xx2-xx1])) * (np.max([0, yy2-yy1]))　
    # 计算交并比
	iou = inter_area / (area1+area2-inter_area+1e-6)
	
	return iou
```

### 2.GIOU  Loss

![[公式]](J:\Main Project\Computer-Vison\MyNote\image\equation-1589027488110.svg)

![image-20200513213316444](J:\Main Project\Computer-Vison\MyNote\image\image-20200513213316444.png)

*上面公式的意思是：先计算两个框的最小闭包区域面积* ![[公式]](J:\Main Project\Computer-Vison\MyNote\image\equation-1589027503675.svg) *(通俗理解：**同时包含了预测框和真实框**的最小框的面积)，再计算出IoU，再计算闭包区域中不属于两个框的区域占闭包区域的比重，最后用IoU减去这个比重得到GIoU。*

![image-20200518201105913](J:\Main Project\Computer-Vison\MyNote\image\image-20200518201105913.png)

```python
def Giou(rec1,rec2):
    #分别是第一个矩形左右上下的坐标
    x1,x2,y1,y2 = rec1 
    x3,x4,y3,y4 = rec2
    iou = Iou(rec1,rec2)
    area_C = (max(x1,x2,x3,x4)-min(x1,x2,x3,x4))*(max(y1,y2,y3,y4)-min(y1,y2,y3,y4))
    area_1 = (x2-x1)*(y1-y2)
    area_2 = (x4-x3)*(y3-y4)
    sum_area = area_1 + area_2

    w1 = x2 - x1   #第一个矩形的宽
    w2 = x4 - x3   #第二个矩形的宽
    h1 = y1 - y2
    h2 = y3 - y4
    W = min(x1,x2,x3,x4)+w1+w2-max(x1,x2,x3,x4)    #交叉部分的宽
    H = min(y1,y2,y3,y4)+h1+h2-max(y1,y2,y3,y4)    #交叉部分的高
    Area = W*H    #交叉的面积
    add_area = sum_area - Area    #两矩形并集的面积

    end_area = (area_C - add_area)/area_C    #闭包区域中不属于两个框的区域占闭包区域的比重
    giou = iou - end_area
    return giou
```

### 3.DIoU Loss

![[公式]](J:\Main Project\Computer-Vison\MyNote\image\equation-1589027730926.svg)

其中， ![[公式]](J:\Main Project\Computer-Vison\MyNote\image\equation-1589027763479.svg) ， ![[公式]](J:\Main Project\Computer-Vison\MyNote\image\equation-1589027763480.svg) 分别代表了预测框和真实框的中心点，且 ![[公式]](J:\Main Project\Computer-Vison\MyNote\image\equation-1589027763481.svg) 代表的是计算两个中心点间的欧式距离。 ![[公式]](J:\Main Project\Computer-Vison\MyNote\image\equation-1589027763504.svg) 代表的是能够同时包含预测框和真实框的**最小闭包区域**的对角线距离。

![img](J:\Main Project\Computer-Vison\MyNote\image\v2-1e4b54001c4abdf392fe9d4877c83972_720w.jpg)

```
def Diou(bboxes1, bboxes2):
    rows = bboxes1.shape[0]
    cols = bboxes2.shape[0]
    dious = torch.zeros((rows, cols))
    if rows * cols == 0:#
        return dious
    exchange = False
    if bboxes1.shape[0] > bboxes2.shape[0]:
        bboxes1, bboxes2 = bboxes2, bboxes1
        dious = torch.zeros((cols, rows))
        exchange = True
    # #xmin,ymin,xmax,ymax->[:,0],[:,1],[:,2],[:,3]
    w1 = bboxes1[:, 2] - bboxes1[:, 0]
    h1 = bboxes1[:, 3] - bboxes1[:, 1] 
    w2 = bboxes2[:, 2] - bboxes2[:, 0]
    h2 = bboxes2[:, 3] - bboxes2[:, 1]
    
    area1 = w1 * h1
    area2 = w2 * h2

    center_x1 = (bboxes1[:, 2] + bboxes1[:, 0]) / 2 
    center_y1 = (bboxes1[:, 3] + bboxes1[:, 1]) / 2 
    center_x2 = (bboxes2[:, 2] + bboxes2[:, 0]) / 2
    center_y2 = (bboxes2[:, 3] + bboxes2[:, 1]) / 2

    inter_max_xy = torch.min(bboxes1[:, 2:],bboxes2[:, 2:]) 
    inter_min_xy = torch.max(bboxes1[:, :2],bboxes2[:, :2]) 
    out_max_xy = torch.max(bboxes1[:, 2:],bboxes2[:, 2:]) 
    out_min_xy = torch.min(bboxes1[:, :2],bboxes2[:, :2])

    inter = torch.clamp((inter_max_xy - inter_min_xy), min=0)
    inter_area = inter[:, 0] * inter[:, 1]
    inter_diag = (center_x2 - center_x1)**2 + (center_y2 - center_y1)**2
    outer = torch.clamp((out_max_xy - out_min_xy), min=0)
    outer_diag = (outer[:, 0] ** 2) + (outer[:, 1] ** 2)
    union = area1+area2-inter_area
    dious = inter_area / union - (inter_diag) / outer_diag
    dious = torch.clamp(dious,min=-1.0,max = 1.0)
    if exchange:
        dious = dious.T
    return dious
```

### 4.RepLoss 优化解决密集遮挡问题

![这里写图片描述](J:\Main Project\Computer-Vison\MyNote\image\20180904193957569.png)!



![Repulsion Loss](J:\Main Project\Computer-Vison\MyNote\image\20180904194930895-1589028558743.png)

# 二、目标跟踪

## 1.DeepSort

### 算法工作流程

1.检测器得到bbox

2.卡尔曼滤波**预测**

3.使用匈牙利算法将**预测后的tracks**和**当前帧中的detecions进行匹配**（级联匹配和IOU匹配）

4.卡尔曼滤波**更新**

* Frame 0：检测器检测到了3个detections，当前没有任何tracks，将这3个detections初始化为tracks                                                                                          
* Frame 1：检测器又检测到了3个detections，对于Frame 0中的tracks，先进行预测得到新的tracks，然后使用匈牙利算法将新的tracks与detections进行匹配，得到(track, detection)匹配对，最后用每对中的detection更新对应的track

### 算法参数

| 参数             | 作用                                                         |
| ---------------- | ------------------------------------------------------------ |
| max_dist         | 最近邻距离度量，马氏距离与余弦距离，用在级联匹配的地方，如果大于此距离，就直接忽略，不进行跟踪 |
| min_confidences  | 检测阈值，忽略所有低于这个值的目标                           |
| nms_max_overlap  | 最大检测重叠，默认为1.0，既不做筛选，若降低该值，则会筛选IOU重叠大于该值的框 |
| max_iou_distance | IOU距离，当IOU小于一定数值时，不认为是同一个目标，注意结果是1-Iou，max_iou_distance所以越大越好 |
| max_age          | 跟踪目标存活期限，代表一个Track存活期限，默认参数是70帧，对于运动速度快的需要减小该值。级联匹配中的循环次数 |
| n_init           | 在类Track中用于判断当前目标状态是否为confirmed，默认参数是3，即当目标确认三次则分配ID，并且此参数用于限制delete的阈值 |
| nn_budget        | 特征列表的长度，每个ID都有一个特征列表，此特征为ReID提取到的特征，如果超过，删除旧的 |

### 匈牙利算法（Hungarian Algorithm）

匈牙利算法用于解决二分图的**最大匹配**问题

算法步骤（假设矩阵为NxN方阵）：

1. 对于矩阵的每一行，减去其中最小的元素
2. 对于矩阵的每一列，减去其中最小的元素
3. 用最少的水平线或垂直线覆盖矩阵中所有的0
4. 如果线的数量等于N，则找到了最优分配，算法结束，否则进入步骤5
5. 找到没有被任何线覆盖的最小元素，每个**没被线覆盖的行**减去这个元素，每个**被线覆盖的列**加上这个元素，返回步骤3

在DeepSORT中，匈牙利算法用来将前一帧中的**跟踪框tracks**与当前帧中的**检测框detections**进行关联，通过**外观信息（appearance information）**和**马氏距离（Mahalanobis distance）**或者余弦距离，或者**IOU**来计算代价矩阵。

### 卡尔曼滤波（Kalman Filter）

卡尔曼滤波分为两个阶段：(1) **预测**track在下一时刻的位置，(2) 基于detection来**更新**预测的位置。

### 余弦距离

余弦距离=1-余弦相似度

## 2.MOTDT

其**核心思想**是同时从Object detection和object tracking里同时生成object candidates (bbox)，设计一种评分机制以选择最终的Candidates（如下图）。这里的intuition是detection和tracking是相辅相成的，比如detection里的high confidence结果可以防止tracking drifts，而tracking可以降低detection带来的偶然的不准确性。

<img src="J:\Main Project\Computer-Vison\MyNote\image\v2-6d90d43638dd0807f69c95845254524c_b.jpg" alt="img" style="zoom:80%;" />

## 3.ReID

### Mutiple Granularity Network

多重粒度网络

MGN是一个多分支的深度网络：**一个全局特征表示**的分支和**两个局部特征表示**的分支。

![img](J:\Main Project\Computer-Vison\MyNote\image\10460403-995281659bcd053d.webp)

不同的N表示不同的粒度，N越大粒度越细。中间的分支N=2，可以理解为将行人分为上半身和下半身；下面的分支N=3，可以理解为将行人分为上，中，下三个部分。









