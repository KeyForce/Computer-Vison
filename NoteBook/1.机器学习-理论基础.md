# 机器学习

## SVM

### **SVM如何处理多分类问题？**

* 间接法：主要是通过组合多个二分类器来实现多分类器的构造

  [`SVC`](https://www.studyai.cn/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) 和 [`NuSVC`](https://www.studyai.cn/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC) 实现了 “one-against-one” 方法用于解决多类别分类问题。 如果 n_class 是类的数量，那么总共需要构建 **n_class * (n_class - 1) / 2** 个分类器，其中每一个分类器都是在数据上训练得到的两类分类器。

### **为什么SVM要引入核函数？**

当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

### 支持向量机的分类？

- 线性可分支持向量机
  - 当训练数据**线性可分**时，通过**硬间隔最大化**，学习一个线性分类器，即线性可分支持向量机，又称**硬间隔支持向量机**。
- 线性支持向量机
  - 当训练数据**接近线性可分**时，通过**软间隔最大化**，学习一个线性分类器，即线性支持向量机，又称**软间隔支持向量机**。
- 非线性支持向量机
  - 当训练数据**线性不可分**时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机。

### SVM核函数有哪些？

- 线性（Linear）核函数：主要用于线性可分的情形。参数少，速度快。
- 多项式核函数
- 高斯（RBF）核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。
- Sigmoid核函数
- 拉普拉斯（Laplac）核函数

注：如果feature数量很大，跟样本数量差不多，建议使用LR或者Linear kernel的SVM。如果feature数量较少，样本数量一般，建议使用Gaussian Kernel的SVM。

## K-近邻算法（KNN）

就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类

* 计算测试数据与各个训练数据之间的距离
* 按照距离的递增关系进行排序
* 选取距离最小的K个点
* 确定前K个点所在类别的出现频率
* 返回前K个点中出现频率最高的类别作为测试数据的预测分类